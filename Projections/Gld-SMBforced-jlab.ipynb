{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward projections with SERMeQ in Jupyter Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should replicate all the functionality of Greenland-SMBforced-autonetworks.py, split into cells to be easily runnable in Jupyter Lab, with the option to add explanatory text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward projections with calving flux on Greenland glaciers, forced by surface mass balance\n",
    "# Sept 2018  EHU\n",
    "# Oct 2019 Jupyter notebook format  EHU\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import mpl_toolkits.basemap.pyproj as pyproj\n",
    "import pyproj\n",
    "import datetime\n",
    "#from matplotlib.colors import LogNorm\n",
    "from matplotlib import cm\n",
    "#from shapely.geometry import *\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import gaussian_filter\n",
    "## Special import for SERMeQ modules\n",
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\ChadSalad\\Documents\\GitHub\\plastic-networks')\n",
    "from SERMeQ.plastic_utilities_v2 import *\n",
    "from SERMeQ.GL_model_tools import *\n",
    "from SERMeQ.flowline_class_hierarchy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in surface topography\n"
     ]
    }
   ],
   "source": [
    "##-------------------\n",
    "### READING IN BED\n",
    "### COMMENT OUT IF DATA IS ALREADY READ IN TO YOUR SESSION\n",
    "##-------------------\n",
    "\n",
    "print 'Reading in surface topography'\n",
    "gl_bed_path ='C:\\\\Users\\ChadSalad\\Documents\\GitHub\\Data_unsynced\\BedMachine-Greenland\\BedMachineGreenland-2017-09-20.nc'\n",
    "fh = Dataset(gl_bed_path, mode='r')\n",
    "xx = fh.variables['x'][:].copy() #x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() #y-coord\n",
    "s_raw = fh.variables['surface'][:].copy() #surface elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_raw=fh.variables['thickness'][:].copy() # Gridded thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_raw = fh.variables['bed'][:].copy() # bed topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thick_mask = fh.variables['mask'][:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.ma.masked_where(thick_mask !=2, s_raw)#mask values: 0=ocean, 1=ice-free land, 2=grounded ice, 3=floating ice, 4=non-Greenland land\n",
    "hh = np.ma.masked_where(thick_mask !=2, h_raw) \n",
    "bb = np.ma.masked_where(thick_mask !=2, b_raw)\n",
    "## Down-sampling\n",
    "X = xx[::2]\n",
    "Y = yy[::2]\n",
    "S = ss[::2, ::2]\n",
    "H = hh[::2, ::2] \n",
    "B = bb[::2, ::2]\n",
    "M = thick_mask[::2,::2]\n",
    "## Not down-sampling\n",
    "#X = xx\n",
    "#Y = yy\n",
    "#S = ss\n",
    "fh.close()\n",
    "\n",
    "#Smoothing bed\n",
    "unsmoothB = B\n",
    "smoothB = gaussian_filter(B, 2)\n",
    "smoothS = gaussian_filter(S, 2) #17 Jan 19 - smoothing S as well for consistency with auto-selected networks\n",
    "#B_processed = np.ma.masked_where(thick_mask !=2, smoothB)\n",
    "\n",
    "S_interp = interpolate.RectBivariateSpline(X, Y[::-1], smoothS.T[::, ::-1])\n",
    "H_interp = interpolate.RectBivariateSpline(X, Y[::-1], H.T[::, ::-1])\n",
    "B_interp = interpolate.RectBivariateSpline(X, Y[::-1], smoothB.T[::, ::-1])\n",
    "def NearestMaskVal(x0,y0):\n",
    "    x_idx = np.abs(X-x0).argmin()\n",
    "    y_idx = np.abs(Y-y0).argmin()\n",
    "    return M[y_idx, x_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in surface mass balance from 1981-2010 climatology\n"
     ]
    }
   ],
   "source": [
    "print 'Reading in surface mass balance from 1981-2010 climatology'\n",
    "gl_smb_path ='C:\\\\Users\\ChadSalad\\Documents\\GitHub\\Data_unsynced\\HIRHAM5-SMB\\DMI-HIRHAM5_GL2_ERAI_1980_2014_SMB_YM.nc'\n",
    "fh2 = Dataset(gl_smb_path, mode='r')\n",
    "x_lon = fh2.variables['lon'][:].copy() #x-coord (latlon)\n",
    "y_lat = fh2.variables['lat'][:].copy() #y-coord (latlon)\n",
    "#zs = fh2.variables['height'][:].copy() #height in m - is this surface elevation or SMB?\n",
    "ts = fh2.variables['time'][:].copy()\n",
    "smb_raw = fh2.variables['smb'][:].copy()\n",
    "fh2.close()\n",
    "\n",
    "#print 'Reading in RCP 4.5 SMB'\n",
    "#gl_smb_2081_path = 'Documents/GitHub/Data_unsynced/DMI-HIRHAM5_G6s2_ECEARTH_RCP45_2081_2100_gld_YM.nc'\n",
    "#fh3 = Dataset(gl_smb_2081_path, mode='r')\n",
    "#x_lon_81 = fh3.variables['lon'][:].copy() #x-coord (latlon)\n",
    "#y_lat_81 = fh3.variables['lat'][:].copy() #y-coord (latlon)\n",
    "##zs = fh2.variables['height'][:].copy() #height in m - is this surface elevation or SMB?\n",
    "#ts_81 = fh3.variables['time'][:].copy()\n",
    "#smb_2081_raw = fh3.variables['gld'][:].copy() #acc SMB in mm/day weq...need to convert\n",
    "#fh3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now transforming coordinate system of SMB\n"
     ]
    }
   ],
   "source": [
    "print 'Now transforming coordinate system of SMB'\n",
    "wgs84 = pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by GPS units and Google Earth\n",
    "psn_gl = pyproj.Proj(\"+init=epsg:3413\") # Polar Stereographic North used by BedMachine (as stated in NetDCF header)\n",
    "xs, ys = pyproj.transform(wgs84, psn_gl, x_lon, y_lat)\n",
    "#xs_81, ys_81 = pyproj.transform(wgs84, psn_gl, x_lon_81, y_lat_81)\n",
    "\n",
    "#Xs = xs[0:,] #flattening; note that x-dimension is 402 according to file header\n",
    "#Ys = ys[:,0] #flattening; note that y-dimension is 602 according to file header\n",
    "\n",
    "smb_1980 = smb_raw[0][0]\n",
    "smb_2014 = smb_raw[-1][0]\n",
    "#smb_init_interpolated = interpolate.interp2d(ys, xs, smb_init, kind='linear')\n",
    "Xmat, Ymat = np.meshgrid(X, Y)\n",
    "regridded_smb_1980 = interpolate.griddata((xs.ravel(), ys.ravel()), smb_1980.ravel(), (Xmat, Ymat), method='nearest')\n",
    "regridded_smb_2014 = interpolate.griddata((xs.ravel(), ys.ravel()), smb_2014.ravel(), (Xmat, Ymat), method='nearest')\n",
    "SMB_1980 = interpolate.interp2d(X, Y, regridded_smb_1980, kind='linear')\n",
    "SMB_2014 = interpolate.interp2d(X, Y, regridded_smb_2014, kind='linear')\n",
    "   \n",
    "#smb_2081_rcp4pt5 = smb_2081_raw[0]\n",
    "#smb_2100_rcp4pt5 = smb_2081_raw[-1] #2100\n",
    "#regridded_smb_2081 = interpolate.griddata((xs_81.ravel(), ys_81.ravel()), smb_2081_rcp4pt5.ravel(), (Xmat, Ymat), method='nearest')\n",
    "#regridded_smb_2100 = interpolate.griddata((xs_81.ravel(), ys_81.ravel()), smb_2100_rcp4pt5.ravel(), (Xmat, Ymat), method='nearest')\n",
    "#SMB_2081_RCP4pt5 = interpolate.interp2d(X, Y, regridded_smb_2081, kind='linear')\n",
    "#SMB_2100_RCP4pt5 = interpolate.interp2d(X, Y, regridded_smb_2100, kind='linear')\n",
    "\n",
    "## Add RCP 8.5 when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in optimal yield strength dictionary\n"
     ]
    }
   ],
   "source": [
    "##-------------------\n",
    "### LOADING SAVED GLACIERS\n",
    "##-------------------\n",
    "print 'Reading in optimal yield strength dictionary'\n",
    "optimal_taus_fpath = 'C:\\\\Users\\ChadSalad\\Documents\\GitHub\\Data_unsynced\\Auto_selected-networks\\Optimization_analysis\\\\bestfit_taus-B_S_smoothing-fromdate_2019-01-17.csv'\n",
    "f_ot = open(optimal_taus_fpath, 'r')\n",
    "header = f_ot.readline()\n",
    "hdr = header.strip('\\r\\n')\n",
    "optimal_taus = {}\n",
    "lines = f_ot.readlines()\n",
    "for i, l in enumerate(lines):\n",
    "    linstrip = l.strip('\\r\\n')\n",
    "    parts = linstrip.split(',')\n",
    "    gid = int(parts[0]) #MEaSUREs glacier ID is first\n",
    "    tau_y = float(parts[1]) #yield strength in Pa\n",
    "    yieldtype = parts[2] #'constant' or 'variable' string\n",
    "    optimal_taus[gid] = (tau_y, yieldtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------------------\n",
    "### SIMULATING GLACIER NETWORKS\n",
    "##-------------------\n",
    "\n",
    "## Define which glaciers are in the simulated set\n",
    "glacier_ids = range(1,195) #MEaSUREs glacier IDs to process.\n",
    "not_present = (93, 94, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 169) #glacier IDs missing from set\n",
    "added_jan19 = (139, 140, 141, 142, 143, 159, 161, 172, 173, 177)\n",
    "errors = (5, 18, 19, 29, 71, 92, 95, 97, 100, 101, 102, 106, 107, 108, 110, 113, 117, 120, 121, 134, 168, 171) #glacier IDs that crashed in hindcasting 12 Mar 2019 *or* showed network problems 21 May 2019\n",
    "rmv = np.concatenate((not_present, errors))\n",
    "for n in rmv:\n",
    "    try:\n",
    "        glacier_ids.remove(n)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "base_fpath = 'C:\\\\Users\\ChadSalad\\Documents\\GitHub\\Data_unsynced\\Auto_selected-networks\\Gld-autonetwork-GID'\n",
    "\n",
    "## Simulation settings\n",
    "testyears = arange(0, 100.1, 0.25)\n",
    "start_year=2006 #determined by which MEaSUREs termini we used to initialize a given set\n",
    "branch_sep_buffer = 10000/L0 #buffer between tributary intersections\n",
    "db = True\n",
    "#test_A, icetemp = 1.7E-24, 'min2C' # -2 C, warm ice\n",
    "test_A, icetemp = 3.5E-25, 'min10C' # -10 C, good guess for Greenland\n",
    "#test_A, icetemp = 3.7E-26, 'min30C' #-30 C, cold ice that should show slower response\n",
    "\n",
    "scenario, SMB_i, SMB_l = 'persistence', SMB_2014, SMB_2014 #choose climate scenario - persistence of 1981-2014 climatology\n",
    "#scenario, SMB_i, SMB_l = 'RCP4pt5', SMB_2014, SMB_2100_RCP4pt5 #or RCP 4.5\n",
    "#scenario, SMB_i, SMB_l = 'RCP8pt5', SMB_2014, SMB_2100_RCP8pt5 #or RCP 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gids_totest = glacier_ids #test all\n",
    "gids_totest = range(11,25) #test a subset\n",
    "#gids_totest = (50, 100, 155, 179) #test a geographically distributed subset\n",
    "# gids_totest = (3,) #test a single glacier or specific selection\n",
    "network_output = []\n",
    "\n",
    "for gid in gids_totest:\n",
    "    if gid in rmv:\n",
    "        print('Glacier {} skipped due to previously identified error.'.format(gid))\n",
    "        continue\n",
    "    print 'Reading in glacier ID: '+str(gid)\n",
    "    if gid in added_jan19:\n",
    "        filename = base_fpath+str(gid)+'-date_2019-01-10.csv'\n",
    "    elif gid<160:\n",
    "        filename = base_fpath+str(gid)+'-date_2018-10-03.csv'\n",
    "    else:\n",
    "        filename = base_fpath+str(gid)+'-date_2018-10-04.csv' #workaround because I ran these in batches and saved them with the date\n",
    "    \n",
    "    coords_list = Flowline_CSV(filename, has_width=True, flip_order=False)\n",
    "    nlines = len(coords_list)\n",
    "    branch_0 = Branch(coords=coords_list[0], index=0, order=0) #saving central branch as main\n",
    "    branch_list = [branch_0]\n",
    "    if nlines>0:\n",
    "        for l in range(1, nlines):\n",
    "            branch_l = Branch(coords = coords_list[l], index=l, order=1, flows_to=0)\n",
    "            branch_list.append(branch_l)\n",
    "    nw = PlasticNetwork(name='GID'+str(gid), init_type='Branch', branches=branch_list, main_terminus=branch_0.coords[0])\n",
    "    nw.make_full_lines()\n",
    "\n",
    "    print 'Now processing glacier ID: '+str(gid)\n",
    "    nw.process_full_lines(B_interp, S_interp, H_interp)\n",
    "    nw.remove_floating()\n",
    "    nw.make_full_lines()\n",
    "    nw.process_full_lines(B_interp, S_interp, H_interp)\n",
    "\n",
    "    print 'Now finding BedMachine terminus'\n",
    "    idx, term_bm = next((i,c) for i,c in enumerate(nw.flowlines[0].coords) if NearestMaskVal(c[0], c[1])==2) #need to do error handling in case this is nowhere satisfied\n",
    "    print 'BedMachine terminus is at index {}, coords {}'.format(idx, term_bm)\n",
    "    #idx, term_bm = next((i, c) for i, c in enumerate(nw.flowlines[0].coords) if S_interp(c[0], c[1])>2.0) #testing with surface cutoff (2m) for now instead of retrieving mask\n",
    "    term_arcval = ArcArray(nw.flowlines[0].coords)[idx]\n",
    "    term_bed = nw.flowlines[0].bed_function(term_arcval)\n",
    "    term_surface = nw.flowlines[0].surface_function(term_arcval)\n",
    "    \n",
    "    nw.network_tau = optimal_taus[gid][0]\n",
    "    nw.network_yield_type = optimal_taus[gid][1]\n",
    "    nw.network_ref_profiles()\n",
    "    \n",
    "    ## Simulations forced by SMB\n",
    "    gl_smb_init = [0.001*SMB_i(nw.flowlines[0].coords[i,0], nw.flowlines[0].coords[i,1]) for i in range(len(nw.flowlines[0].coords))]\n",
    "    nw.smb_alphadot = np.mean(gl_smb_init) #work on summing over all branches later\n",
    "    print 'a_dot from SMB: {}'.format(nw.smb_alphadot)\n",
    "    nw.terminus_adot = gl_smb_init[0]\n",
    "    print 'Terminus a_dot: {}'.format(nw.terminus_adot)\n",
    "    #gl.sec_mainline = np.asarray([SEC_i(gl.flowlines[0].coords[i,0], gl.flowlines[0].coords[i,1]) for i in range(len(gl.flowlines[0].coords))])\n",
    "    #away_from_edge = np.argmin(gl.sec_mainline)\n",
    "    #gl.sec_alphadot = np.mean(gl.sec_mainline[away_from_edge::])\n",
    "    #variable_forcing = linspace(start=gl.sec_alphadot, stop=2*gl.sec_alphadot, num=len(testyears))\n",
    "    #gl.terminus_sec = float(min(gl.sec_mainline.flatten()))#using min because values close to edge get disrupted by mask interpolation\n",
    "    #gl.terminus_time_evolve(testyears=testyears, alpha_dot=gl.smb_alphadot, dL=1/L0, separation_buffer=10000/L0, has_smb=True, terminus_balance=gl.terminus_adot, submarine_melt = 0, debug_mode=db, rate_factor=test_A, output_heavy=False) \n",
    "    gl_smb_2100 = [0.001*365.26*SMB_l(nw.flowlines[0].coords[i,0], nw.flowlines[0].coords[i,1]) for i in range(len(nw.flowlines[0].coords))]\n",
    "    nw.smb_2100_alphadot = np.mean(gl_smb_2100)\n",
    "    steps_til_2100 = int((2100-start_year) / mean(diff(testyears))) #length of linspace array that will determine forcing up to 2100 (HIRHAM-5 end 21st Century time)\n",
    "    forcing_til_2100 = linspace(start=nw.smb_alphadot, stop=nw.smb_2100_alphadot, num=steps_til_2100) \n",
    "    if len(testyears)>steps_til_2100:\n",
    "        steps_after_2100 = len(testyears) - steps_til_2100 #length of array that determines forcing after 2100\n",
    "        forcing_after_2100 = np.full(shape=steps_after_2100, fill_value=nw.smb_2100_alphadot) #persist with 2100 SMB for timesteps after 2100\n",
    "        variable_forcing = np.concatenate((forcing_til_2100, forcing_after_2100))\n",
    "    else:\n",
    "        variable_forcing = forcing_til_2100[:len(testyears)]\n",
    "    nw.terminus_time_evolve(testyears=testyears, alpha_dot_variable=variable_forcing, dL=1/L0, separation_buffer=10000/L0, has_smb=True, terminus_balance=nw.terminus_adot, submarine_melt = 0, debug_mode=db, rate_factor=test_A, output_heavy=False)\n",
    "\n",
    "    print 'Saving output for {}'.format(nw.name)\n",
    "    fpath = 'C:\\\\Users\\ChadSalad\\Documents\\GitHub\\Data_unsynced\\SERMeQ_output\\\\'\n",
    "    fn = str(nw.name)\n",
    "    fn1 = fn.replace(\" \", \"\")\n",
    "    fn2 = fn1.replace(\"[\", \"-\")\n",
    "    fn3 = fn2.replace(\"/\", \"_\")\n",
    "    fn4 = fn3.replace(\"]\", \"\")\n",
    "    fn5 = fn4+'-{}-{}-{}ice-{}a_dt025a.pickle'.format(datetime.date.today(), scenario, icetemp, int(max(testyears)))\n",
    "    nw.save_network(filename=fpath+fn5)\n",
    "    \n",
    "    #network_output.append(nw.model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
